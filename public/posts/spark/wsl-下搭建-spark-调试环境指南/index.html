<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>WSL 下搭建 Spark 调试环境指南 | CodeMist · 记录技术与生活</title><meta name=keywords content="WSL,Spark,Hadoop"><meta name=description content="这是一篇记录在 WSL 环境下搭建 Spark 调试环境时遇到的问题及其解决方法的博客，方便日后排雷复用。"><meta name=author content="CodeMist"><link rel=canonical href=https://codeofwhite.github.io/posts/spark/wsl-%E4%B8%8B%E6%90%AD%E5%BB%BA-spark-%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83%E6%8C%87%E5%8D%97/><link crossorigin=anonymous href=/assets/css/stylesheet.6da9a63d25a9608bca2f7f907a030e887a7dd3c3f3918e4cc113129361414bda.css integrity rel="preload stylesheet" as=style><link rel=icon href=https://codeofwhite.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://codeofwhite.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://codeofwhite.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://codeofwhite.github.io/apple-touch-icon.png><link rel=mask-icon href=https://codeofwhite.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://codeofwhite.github.io/posts/spark/wsl-%E4%B8%8B%E6%90%AD%E5%BB%BA-spark-%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83%E6%8C%87%E5%8D%97/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://codeofwhite.github.io/posts/spark/wsl-%E4%B8%8B%E6%90%AD%E5%BB%BA-spark-%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83%E6%8C%87%E5%8D%97/"><meta property="og:site_name" content="CodeMist · 记录技术与生活"><meta property="og:title" content="WSL 下搭建 Spark 调试环境指南"><meta property="og:description" content="这是一篇记录在 WSL 环境下搭建 Spark 调试环境时遇到的问题及其解决方法的博客，方便日后排雷复用。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-26T00:00:00+00:00"><meta property="article:tag" content="WSL"><meta property="article:tag" content="Spark"><meta property="article:tag" content="Hadoop"><meta name=twitter:card content="summary"><meta name=twitter:title content="WSL 下搭建 Spark 调试环境指南"><meta name=twitter:description content="这是一篇记录在 WSL 环境下搭建 Spark 调试环境时遇到的问题及其解决方法的博客，方便日后排雷复用。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://codeofwhite.github.io/posts/"},{"@type":"ListItem","position":2,"name":"WSL 下搭建 Spark 调试环境指南","item":"https://codeofwhite.github.io/posts/spark/wsl-%E4%B8%8B%E6%90%AD%E5%BB%BA-spark-%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83%E6%8C%87%E5%8D%97/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"WSL 下搭建 Spark 调试环境指南","name":"WSL 下搭建 Spark 调试环境指南","description":"这是一篇记录在 WSL 环境下搭建 Spark 调试环境时遇到的问题及其解决方法的博客，方便日后排雷复用。\n","keywords":["WSL","Spark","Hadoop"],"articleBody":"这是一篇记录在 WSL 环境下搭建 Spark 调试环境时遇到的问题及其解决方法的博客，方便日后排雷复用。\nBug 1：用户权限问题 启动 Hadoop 时出现错误：\nERROR: namenode can only be executed by root. 排查后发现，start-dfs.sh 脚本中对执行用户有限制（如下图），当前用户非 root 时会报错：\n解决方法：\n使用 root 用户执行启动脚本，或 修改 start-dfs.sh 中的用户校验（仅建议在开发环境使用） Bug 2：SSH 无密码登录失败 报错信息：\nlocalhost: zhj20@localhost: Permission denied (publickey,password) 说明 SSH 无密码登录未配置，Hadoop 启动过程中的内部通信失败。\n解决步骤：\n生成 SSH 密钥（若无）\nssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa 配置公钥认证\ncat ~/.ssh/id_rsa.pub \u003e\u003e ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys 验证是否能免密登录\nssh localhost 成功后应自动登录，无需输入密码，exit 退出即可。\nBug 3：YARN 节点列表为空 执行 yarn node -list 时无任何节点。\n排查方向：\n确认 ResourceManager 和 NodeManager 是否都已启动（jps 查看） 检查 yarn-site.xml 配置是否正确 查看日志文件是否有网络连接/端口异常 Bug 4：NameNode 启动失败（EditLog 异常） There appears to be a gap in the edit log. We expected txid 1, but got txid 113. 原因分析：\nHadoop 恢复 fsimage + editlog 时发现 txid 序号不连续，editlog 可能损坏或缺失。 解决方案（无重要数据时）：\nhdfs namenode -format ./sbin/start-dfs.sh 注意：此操作会清空 HDFS 中的所有数据，仅建议在调试或开发环境中使用！\nBug 5：DataNode 启动失败，无法写入副本 错误信息：\nFile ... could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running 排查配置：\ncat $HADOOP_HOME/etc/hadoop/hdfs-site.xml | grep -A 5 dfs.datanode.data.dir 日志提示：\nIncompatible clusterIDs namenode clusterID = CID-018939f5... datanode clusterID = CID-659a4b77... 说明 DataNode 保存的 clusterID 与 NameNode 不一致。\n解决办法：\n# 停止 Hadoop ./sbin/stop-dfs.sh # 清除 DataNode 数据（不要删 NameNode 数据！） rm -rf /opt/module/hadoop-3.4.1/hadoop_data/hdfs/datanode/* # 重启 ./sbin/start-dfs.sh # 查看是否正常启动 jps 参考链接 WSL 配置 Spark 的坑（ChatGPT 分享页 1） 调试 HDFS NameNode 启动失败（ChatGPT 分享页 2） DataNode ClusterID 不一致排查记录（ChatGPT 分享页 3） ","wordCount":"211","inLanguage":"en","datePublished":"2025-04-26T00:00:00Z","dateModified":"2025-04-26T00:00:00Z","author":{"@type":"Person","name":"CodeMist"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codeofwhite.github.io/posts/spark/wsl-%E4%B8%8B%E6%90%AD%E5%BB%BA-spark-%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83%E6%8C%87%E5%8D%97/"},"publisher":{"@type":"Organization","name":"CodeMist · 记录技术与生活","logo":{"@type":"ImageObject","url":"https://codeofwhite.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://codeofwhite.github.io/ accesskey=h title="CodeMist · 记录技术与生活 (Alt + H)">CodeMist · 记录技术与生活</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://codeofwhite.github.io/posts/ title=文章><span>文章</span></a></li><li><a href=https://codeofwhite.github.io/categories/bigdata/ title=大数据><span>大数据</span></a></li><li><a href=https://codeofwhite.github.io/categories/ai/ title=AI><span>AI</span></a></li><li><a href=https://codeofwhite.github.io/categories/daily/ title="学习 · 日常"><span>学习 · 日常</span></a></li><li><a href=https://codeofwhite.github.io/about/ title=关于><span>关于</span></a></li><li><a href=https://codeofwhite.github.io/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://codeofwhite.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://codeofwhite.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">WSL 下搭建 Spark 调试环境指南</h1><div class=post-meta><span title='2025-04-26 00:00:00 +0000 UTC'>2025-04-26</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;CodeMist</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#bug-1%e7%94%a8%e6%88%b7%e6%9d%83%e9%99%90%e9%97%ae%e9%a2%98 aria-label="Bug 1：用户权限问题">Bug 1：用户权限问题</a></li><li><a href=#bug-2ssh-%e6%97%a0%e5%af%86%e7%a0%81%e7%99%bb%e5%bd%95%e5%a4%b1%e8%b4%a5 aria-label="Bug 2：SSH 无密码登录失败">Bug 2：SSH 无密码登录失败</a></li><li><a href=#bug-3yarn-%e8%8a%82%e7%82%b9%e5%88%97%e8%a1%a8%e4%b8%ba%e7%a9%ba aria-label="Bug 3：YARN 节点列表为空">Bug 3：YARN 节点列表为空</a></li><li><a href=#bug-4namenode-%e5%90%af%e5%8a%a8%e5%a4%b1%e8%b4%a5editlog-%e5%bc%82%e5%b8%b8 aria-label="Bug 4：NameNode 启动失败（EditLog 异常）">Bug 4：NameNode 启动失败（EditLog 异常）</a></li><li><a href=#bug-5datanode-%e5%90%af%e5%8a%a8%e5%a4%b1%e8%b4%a5%e6%97%a0%e6%b3%95%e5%86%99%e5%85%a5%e5%89%af%e6%9c%ac aria-label="Bug 5：DataNode 启动失败，无法写入副本">Bug 5：DataNode 启动失败，无法写入副本</a></li><li><a href=#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5 aria-label=参考链接>参考链接</a></li></ul></div></details></div><div class=post-content><p>这是一篇记录在 WSL 环境下搭建 Spark 调试环境时遇到的问题及其解决方法的博客，方便日后排雷复用。</p><h2 id=bug-1用户权限问题>Bug 1：用户权限问题<a hidden class=anchor aria-hidden=true href=#bug-1用户权限问题>#</a></h2><p>启动 Hadoop 时出现错误：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ERROR: namenode can only be executed by root.
</span></span></code></pre></div><p>排查后发现，<code>start-dfs.sh</code> 脚本中对执行用户有限制（如下图），当前用户非 root 时会报错：</p><p><img alt="alt text" loading=lazy src=/images/image.png></p><p><strong>解决方法：</strong></p><ul><li>使用 root 用户执行启动脚本，或</li><li>修改 <code>start-dfs.sh</code> 中的用户校验（仅建议在开发环境使用）</li></ul><hr><h2 id=bug-2ssh-无密码登录失败>Bug 2：SSH 无密码登录失败<a hidden class=anchor aria-hidden=true href=#bug-2ssh-无密码登录失败>#</a></h2><p>报错信息：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>localhost: zhj20@localhost: Permission denied <span class=o>(</span>publickey,password<span class=o>)</span>
</span></span></code></pre></div><p>说明 SSH 无密码登录未配置，Hadoop 启动过程中的内部通信失败。</p><p><strong>解决步骤：</strong></p><ol><li><p><strong>生成 SSH 密钥（若无）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh-keygen -t rsa -P <span class=s1>&#39;&#39;</span> -f ~/.ssh/id_rsa
</span></span></code></pre></div></li><li><p><strong>配置公钥认证</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</span></span><span class=line><span class=cl>chmod <span class=m>600</span> ~/.ssh/authorized_keys
</span></span></code></pre></div></li><li><p><strong>验证是否能免密登录</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh localhost
</span></span></code></pre></div></li></ol><p>成功后应自动登录，无需输入密码，<code>exit</code> 退出即可。</p><hr><h2 id=bug-3yarn-节点列表为空>Bug 3：YARN 节点列表为空<a hidden class=anchor aria-hidden=true href=#bug-3yarn-节点列表为空>#</a></h2><p>执行 <code>yarn node -list</code> 时无任何节点。</p><p><strong>排查方向：</strong></p><ul><li>确认 ResourceManager 和 NodeManager 是否都已启动（<code>jps</code> 查看）</li><li>检查 <code>yarn-site.xml</code> 配置是否正确</li><li>查看日志文件是否有网络连接/端口异常</li></ul><hr><h2 id=bug-4namenode-启动失败editlog-异常>Bug 4：NameNode 启动失败（EditLog 异常）<a hidden class=anchor aria-hidden=true href=#bug-4namenode-启动失败editlog-异常>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>There appears to be a gap in the edit log. We expected txid 1, but got txid 113.
</span></span></code></pre></div><p><strong>原因分析：</strong></p><ul><li>Hadoop 恢复 fsimage + editlog 时发现 txid 序号不连续，editlog 可能损坏或缺失。</li></ul><p><strong>解决方案（无重要数据时）：</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hdfs namenode -format
</span></span><span class=line><span class=cl>./sbin/start-dfs.sh
</span></span></code></pre></div><p>注意：此操作会清空 HDFS 中的所有数据，仅建议在调试或开发环境中使用！</p><hr><h2 id=bug-5datanode-启动失败无法写入副本>Bug 5：DataNode 启动失败，无法写入副本<a hidden class=anchor aria-hidden=true href=#bug-5datanode-启动失败无法写入副本>#</a></h2><p>错误信息：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>File ... could only be written to <span class=m>0</span> of the <span class=m>1</span> minReplication nodes. There are <span class=m>0</span> datanode<span class=o>(</span>s<span class=o>)</span> running
</span></span></code></pre></div><p>排查配置：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cat <span class=nv>$HADOOP_HOME</span>/etc/hadoop/hdfs-site.xml <span class=p>|</span> grep -A <span class=m>5</span> dfs.datanode.data.dir
</span></span></code></pre></div><p>日志提示：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Incompatible clusterIDs
</span></span><span class=line><span class=cl>namenode <span class=nv>clusterID</span> <span class=o>=</span> CID-018939f5...
</span></span><span class=line><span class=cl>datanode <span class=nv>clusterID</span> <span class=o>=</span> CID-659a4b77...
</span></span></code></pre></div><p>说明 DataNode 保存的 clusterID 与 NameNode 不一致。</p><p><strong>解决办法：</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 停止 Hadoop</span>
</span></span><span class=line><span class=cl>./sbin/stop-dfs.sh
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 清除 DataNode 数据（不要删 NameNode 数据！）</span>
</span></span><span class=line><span class=cl>rm -rf /opt/module/hadoop-3.4.1/hadoop_data/hdfs/datanode/*
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 重启</span>
</span></span><span class=line><span class=cl>./sbin/start-dfs.sh
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看是否正常启动</span>
</span></span><span class=line><span class=cl>jps
</span></span></code></pre></div><hr><h2 id=参考链接>参考链接<a hidden class=anchor aria-hidden=true href=#参考链接>#</a></h2><ul><li><a href=https://chatgpt.com/share/680c4e86-85fc-800f-917c-db92b4a6e81c>WSL 配置 Spark 的坑（ChatGPT 分享页 1）</a></li><li><a href=https://chatgpt.com/share/680c4ead-ee00-800f-ab1d-47c1235f3bf0>调试 HDFS NameNode 启动失败（ChatGPT 分享页 2）</a></li><li><a href=https://chatgpt.com/share/680c4ebc-5f4c-800f-a2ef-247f8d461413>DataNode ClusterID 不一致排查记录（ChatGPT 分享页 3）</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://codeofwhite.github.io/tags/wsl/>WSL</a></li><li><a href=https://codeofwhite.github.io/tags/spark/>Spark</a></li><li><a href=https://codeofwhite.github.io/tags/hadoop/>Hadoop</a></li></ul><nav class=paginav><a class=prev href=https://codeofwhite.github.io/posts/spark/spark-%E7%9A%84%E5%87%A0%E7%A7%8D%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F/><span class=title>« Prev</span><br><span>Spark 的几种启动方式</span>
</a><a class=next href=https://codeofwhite.github.io/posts/spark/wsl-%E9%85%8D%E7%BD%AE-spark/><span class=title>Next »</span><br><span>WSL 配置 Spark</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://codeofwhite.github.io/>CodeMist · 记录技术与生活</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script data-goatcounter=https://codeofwhite.goatcounter.com/count async src=//gc.zgo.at/count.js></script></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>