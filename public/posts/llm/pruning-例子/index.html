<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pruning 例子 | CodeMist · 记录技术与生活</title><meta name=keywords content="LLM"><meta name=description content="LeNet(
  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
在pytorch里面, prune通过对权重进行掩码来完成. 这个如何理解?
首先, 我们打印一下原始的conv1的权重看看:
module = model.conv1
print(list(module.named_modules()))
print(list(module.named_buffers()))
print(list(module.named_parameters()))
这里列举了后面可能会用到三个方法, 这个可以查看当前的module到底是一个啥情况。
着重观察 named_parameters 因为参数都保存在这里, 打印完了之后可以看到:
[('weight', Parameter containing:
tensor([[[[-0.2312,  0.2133, -0.1313],
          [-0.2980, -0.1838, -0.2902],
          [-0.3006,  0.1338, -0.0980]]],
        [[[-0.1239,  0.1060,  0.3271],
          [-0.0301, -0.0245,  0.0493],
          [-0.0160,  0.0397, -0.1242]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:
tensor([-0.2593, -0.0520,  0.0303,  0.0382, -0.0468, -0.1053], device='cuda:0',
       requires_grad=True))]
它有一个weight和一个bias, 这没错, 合乎常理. 我们甚至可以看看weights的尺寸是多少."><meta name=author content="CodeMist"><link rel=canonical href=https://codeofwhite.github.io/posts/llm/pruning-%E4%BE%8B%E5%AD%90/><link crossorigin=anonymous href=/assets/css/stylesheet.6da9a63d25a9608bca2f7f907a030e887a7dd3c3f3918e4cc113129361414bda.css integrity rel="preload stylesheet" as=style><link rel=icon href=https://codeofwhite.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://codeofwhite.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://codeofwhite.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://codeofwhite.github.io/apple-touch-icon.png><link rel=mask-icon href=https://codeofwhite.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://codeofwhite.github.io/posts/llm/pruning-%E4%BE%8B%E5%AD%90/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://codeofwhite.github.io/posts/llm/pruning-%E4%BE%8B%E5%AD%90/"><meta property="og:site_name" content="CodeMist · 记录技术与生活"><meta property="og:title" content="Pruning 例子"><meta property="og:description" content="LeNet( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 在pytorch里面, prune通过对权重进行掩码来完成. 这个如何理解?
首先, 我们打印一下原始的conv1的权重看看:
module = model.conv1 print(list(module.named_modules())) print(list(module.named_buffers())) print(list(module.named_parameters())) 这里列举了后面可能会用到三个方法, 这个可以查看当前的module到底是一个啥情况。
着重观察 named_parameters 因为参数都保存在这里, 打印完了之后可以看到:
[('weight', Parameter containing: tensor([[[[-0.2312, 0.2133, -0.1313], [-0.2980, -0.1838, -0.2902], [-0.3006, 0.1338, -0.0980]]], [[[-0.1239, 0.1060, 0.3271], [-0.0301, -0.0245, 0.0493], [-0.0160, 0.0397, -0.1242]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing: tensor([-0.2593, -0.0520, 0.0303, 0.0382, -0.0468, -0.1053], device='cuda:0', requires_grad=True))] 它有一个weight和一个bias, 这没错, 合乎常理. 我们甚至可以看看weights的尺寸是多少."><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-11T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-11T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta name=twitter:card content="summary"><meta name=twitter:title content="Pruning 例子"><meta name=twitter:description content="LeNet(
  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
在pytorch里面, prune通过对权重进行掩码来完成. 这个如何理解?
首先, 我们打印一下原始的conv1的权重看看:
module = model.conv1
print(list(module.named_modules()))
print(list(module.named_buffers()))
print(list(module.named_parameters()))
这里列举了后面可能会用到三个方法, 这个可以查看当前的module到底是一个啥情况。
着重观察 named_parameters 因为参数都保存在这里, 打印完了之后可以看到:
[('weight', Parameter containing:
tensor([[[[-0.2312,  0.2133, -0.1313],
          [-0.2980, -0.1838, -0.2902],
          [-0.3006,  0.1338, -0.0980]]],
        [[[-0.1239,  0.1060,  0.3271],
          [-0.0301, -0.0245,  0.0493],
          [-0.0160,  0.0397, -0.1242]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:
tensor([-0.2593, -0.0520,  0.0303,  0.0382, -0.0468, -0.1053], device='cuda:0',
       requires_grad=True))]
它有一个weight和一个bias, 这没错, 合乎常理. 我们甚至可以看看weights的尺寸是多少."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://codeofwhite.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Pruning 例子","item":"https://codeofwhite.github.io/posts/llm/pruning-%E4%BE%8B%E5%AD%90/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pruning 例子","name":"Pruning 例子","description":"LeNet( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 在pytorch里面, prune通过对权重进行掩码来完成. 这个如何理解?\n首先, 我们打印一下原始的conv1的权重看看:\nmodule = model.conv1 print(list(module.named_modules())) print(list(module.named_buffers())) print(list(module.named_parameters())) 这里列举了后面可能会用到三个方法, 这个可以查看当前的module到底是一个啥情况。\n着重观察 named_parameters 因为参数都保存在这里, 打印完了之后可以看到:\n[(\u0026#39;weight\u0026#39;, Parameter containing: tensor([[[[-0.2312, 0.2133, -0.1313], [-0.2980, -0.1838, -0.2902], [-0.3006, 0.1338, -0.0980]]], [[[-0.1239, 0.1060, 0.3271], [-0.0301, -0.0245, 0.0493], [-0.0160, 0.0397, -0.1242]]]], device=\u0026#39;cuda:0\u0026#39;, requires_grad=True)), (\u0026#39;bias\u0026#39;, Parameter containing: tensor([-0.2593, -0.0520, 0.0303, 0.0382, -0.0468, -0.1053], device=\u0026#39;cuda:0\u0026#39;, requires_grad=True))] 它有一个weight和一个bias, 这没错, 合乎常理. 我们甚至可以看看weights的尺寸是多少.\n","keywords":["LLM"],"articleBody":"LeNet( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 在pytorch里面, prune通过对权重进行掩码来完成. 这个如何理解?\n首先, 我们打印一下原始的conv1的权重看看:\nmodule = model.conv1 print(list(module.named_modules())) print(list(module.named_buffers())) print(list(module.named_parameters())) 这里列举了后面可能会用到三个方法, 这个可以查看当前的module到底是一个啥情况。\n着重观察 named_parameters 因为参数都保存在这里, 打印完了之后可以看到:\n[('weight', Parameter containing: tensor([[[[-0.2312, 0.2133, -0.1313], [-0.2980, -0.1838, -0.2902], [-0.3006, 0.1338, -0.0980]]], [[[-0.1239, 0.1060, 0.3271], [-0.0301, -0.0245, 0.0493], [-0.0160, 0.0397, -0.1242]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing: tensor([-0.2593, -0.0520, 0.0303, 0.0382, -0.0468, -0.1053], device='cuda:0', requires_grad=True))] 它有一个weight和一个bias, 这没错, 合乎常理. 我们甚至可以看看weights的尺寸是多少.\nfor a in module.named_parameters(): print(a[1].shape) 输出:\ntorch.Size([6, 1, 3, 3]) torch.Size([6]) 这个其实就是卷积的维度了, 6指的是channel, 1值得还是stride, 3指的是kernel size. 然后重点来了, 要开始做prune了, 在pytorch里面操作也很简单, 只需要一行代码:\nimport torch.nn.utils.prune as prune ​ prune.random_unstructured(module, name='weight', amount=0.3) ​ 这个可以从众多的剪枝方法中, 选择一个很好的手段来完成同样的目的. 然后我们再打印一下named_parameters:\n[('bias', Parameter containing: tensor([-0.2281, 0.3085, 0.0937, -0.0540, 0.3295, 0.1107], device='cuda:0', requires_grad=True)), ('weight_orig', Parameter containing: tensor([[[[ 0.1934, -0.0172, -0.1957], [ 0.1655, 0.1669, -0.2448], [-0.2250, -0.0963, -0.0195]]], ​ [[[-0.3154, 0.1868, 0.0103], [-0.2245, 0.1548, 0.2567], [ 0.0713, 0.1262, 0.1547]]]], device='cuda:0', requires_grad=True))] 唯一的变化就是 weights 变成了 weights_orig, prune之后通过掩码的方式存放在了 named_buffers里面:\nprint(list(module.named_buffers())) 可以看到:\n[('weight_mask', tensor([[[[1., 1., 1.], [0., 1., 0.], [0., 1., 1.]]], [[[0., 1., 0.], [0., 0., 1.], [0., 0., 1.]]]], device='cuda:0'))] 那么问题来了, 只是把权重进行了掩码, 那么我要知道你剪掉了哪几个channel怎么办? 而且你这个是剪的权重, 结构呢? 我怎么把这个结构找出来??\n所以说这只是第一步, 接下来我们来看看结构化修剪. ==结构化修剪讲道理你可以知道你修剪了哪些结构==.\n一个比较好的结构化修建的例子是通过沿着Tensor的某个维度进行裁剪, 这样你可以直接看到维度的变化.\n现在开始修剪模块, 比如上面的LeNet的conv1层, 首先我们可以从prune层里面拿一个我们喜欢的技术, 比如基于 ln范数 的评判标准来进行结构话的裁剪.\nprune.ln_structured(module, name='weight', amount=0.5, n=2, dim=0) 这个操作之后, 我们得到的将是一个新的权重, 和上面的非结构化的不同的地方在于, 这里是整个矩阵的一行为零, 上面我们用的dim=0, 那么就是channel这一个维度, 会有50%为零.\n","wordCount":"210","inLanguage":"en","datePublished":"2025-02-11T00:00:00Z","dateModified":"2025-02-11T00:00:00Z","author":{"@type":"Person","name":"CodeMist"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codeofwhite.github.io/posts/llm/pruning-%E4%BE%8B%E5%AD%90/"},"publisher":{"@type":"Organization","name":"CodeMist · 记录技术与生活","logo":{"@type":"ImageObject","url":"https://codeofwhite.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://codeofwhite.github.io/ accesskey=h title="CodeMist · 记录技术与生活 (Alt + H)">CodeMist · 记录技术与生活</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://codeofwhite.github.io/posts/ title=文章><span>文章</span></a></li><li><a href=https://codeofwhite.github.io/categories/bigdata/ title=大数据><span>大数据</span></a></li><li><a href=https://codeofwhite.github.io/categories/ai/ title=AI><span>AI</span></a></li><li><a href=https://codeofwhite.github.io/categories/daily/ title="学习 · 日常"><span>学习 · 日常</span></a></li><li><a href=https://codeofwhite.github.io/about/ title=关于><span>关于</span></a></li><li><a href=https://codeofwhite.github.io/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://codeofwhite.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://codeofwhite.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Pruning 例子</h1><div class=post-meta><span title='2025-02-11 00:00:00 +0000 UTC'>2025-02-11</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;CodeMist</div></header><div class=post-content><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>LeNet(
</span></span><span class=line><span class=cl>  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
</span></span><span class=line><span class=cl>  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
</span></span><span class=line><span class=cl>  (fc1): Linear(in_features=400, out_features=120, bias=True)
</span></span><span class=line><span class=cl>  (fc2): Linear(in_features=120, out_features=84, bias=True)
</span></span><span class=line><span class=cl>  (fc3): Linear(in_features=84, out_features=10, bias=True)
</span></span><span class=line><span class=cl>)
</span></span></code></pre></div><p>在pytorch里面, prune通过对权重进行掩码来完成. 这个如何理解?</p><p>首先, 我们打印一下原始的conv1的权重看看:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>module = model.conv1
</span></span><span class=line><span class=cl>print(list(module.named_modules()))
</span></span><span class=line><span class=cl>print(list(module.named_buffers()))
</span></span><span class=line><span class=cl>print(list(module.named_parameters()))
</span></span></code></pre></div><p>这里列举了后面可能会用到三个方法, 这个可以查看当前的module到底是一个啥情况。</p><p>着重观察 <code>named_parameters</code> 因为参数都保存在这里, 打印完了之后可以看到:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>[(&#39;weight&#39;, Parameter containing:
</span></span><span class=line><span class=cl>tensor([[[[-0.2312,  0.2133, -0.1313],
</span></span><span class=line><span class=cl>          [-0.2980, -0.1838, -0.2902],
</span></span><span class=line><span class=cl>          [-0.3006,  0.1338, -0.0980]]],
</span></span><span class=line><span class=cl>        [[[-0.1239,  0.1060,  0.3271],
</span></span><span class=line><span class=cl>          [-0.0301, -0.0245,  0.0493],
</span></span><span class=line><span class=cl>          [-0.0160,  0.0397, -0.1242]]]], device=&#39;cuda:0&#39;, requires_grad=True)), (&#39;bias&#39;, Parameter containing:
</span></span><span class=line><span class=cl>tensor([-0.2593, -0.0520,  0.0303,  0.0382, -0.0468, -0.1053], device=&#39;cuda:0&#39;,
</span></span><span class=line><span class=cl>       requires_grad=True))]
</span></span></code></pre></div><p>它有一个weight和一个bias, 这没错, 合乎常理. 我们甚至可以看看weights的尺寸是多少.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>for a in module.named_parameters():
</span></span><span class=line><span class=cl>    print(a[1].shape)
</span></span></code></pre></div><p>输出:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>torch.Size([6, 1, 3, 3])
</span></span><span class=line><span class=cl>torch.Size([6])
</span></span></code></pre></div><p>这个其实就是卷积的维度了, 6指的是channel, 1值得还是stride, 3指的是kernel size.
然后重点来了, 要开始做prune了, 在pytorch里面操作也很简单, 只需要一行代码:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>import torch.nn.utils.prune as prune
</span></span><span class=line><span class=cl>​
</span></span><span class=line><span class=cl>prune.random_unstructured(module, name=&#39;weight&#39;, amount=0.3)
</span></span><span class=line><span class=cl>​
</span></span></code></pre></div><p>这个可以从众多的剪枝方法中, 选择一个很好的手段来完成同样的目的. 然后我们再打印一下named_parameters:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>[(&#39;bias&#39;, Parameter containing:
</span></span><span class=line><span class=cl>tensor([-0.2281,  0.3085,  0.0937, -0.0540,  0.3295,  0.1107], device=&#39;cuda:0&#39;,
</span></span><span class=line><span class=cl>       requires_grad=True)), (&#39;weight_orig&#39;, Parameter containing:
</span></span><span class=line><span class=cl>tensor([[[[ 0.1934, -0.0172, -0.1957],
</span></span><span class=line><span class=cl>          [ 0.1655,  0.1669, -0.2448],
</span></span><span class=line><span class=cl>          [-0.2250, -0.0963, -0.0195]]],
</span></span><span class=line><span class=cl>​
</span></span><span class=line><span class=cl>        [[[-0.3154,  0.1868,  0.0103],
</span></span><span class=line><span class=cl>          [-0.2245,  0.1548,  0.2567],
</span></span><span class=line><span class=cl>          [ 0.0713,  0.1262,  0.1547]]]], device=&#39;cuda:0&#39;, requires_grad=True))]
</span></span></code></pre></div><p>唯一的变化就是 <code>weights</code> 变成了 <code>weights_orig</code>, prune之后通过掩码的方式存放在了 <code>named_buffers</code>里面:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>print(list(module.named_buffers()))
</span></span></code></pre></div><p>可以看到:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>[(&#39;weight_mask&#39;, tensor([[[[1., 1., 1.],
</span></span><span class=line><span class=cl>          [0., 1., 0.],
</span></span><span class=line><span class=cl>          [0., 1., 1.]]],
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        [[[0., 1., 0.],
</span></span><span class=line><span class=cl>          [0., 0., 1.],
</span></span><span class=line><span class=cl>          [0., 0., 1.]]]], device=&#39;cuda:0&#39;))]
</span></span></code></pre></div><p>那么问题来了, 只是把权重进行了掩码, 那么我要知道你剪掉了哪几个channel怎么办? 而且你这个是剪的权重, 结构呢? 我怎么把这个结构找出来??</p><p>所以说这只是第一步, 接下来我们来看看结构化修剪. ==结构化修剪讲道理你可以知道你修剪了哪些结构==.</p><p><strong>一个比较好的结构化修建的例子是通过沿着Tensor的某个维度进行裁剪, 这样你可以直接看到维度的变化</strong>.</p><p>现在开始修剪模块, 比如上面的LeNet的conv1层, 首先我们可以从prune层里面拿一个我们喜欢的技术, 比如基于 <strong>ln范数</strong> 的评判标准来进行结构话的裁剪.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>prune.ln_structured(module, name=&#39;weight&#39;, amount=0.5, n=2, dim=0)
</span></span></code></pre></div><p>这个操作之后, 我们得到的将是一个新的权重, 和上面的非结构化的不同的地方在于, 这里是整个矩阵的一行为零, 上面我们用的dim=0, 那么就是channel这一个维度, 会有50%为零.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://codeofwhite.github.io/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://codeofwhite.github.io/posts/%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo-+-github-pages/><span class=title>« Prev</span><br><span>部署个人博客：Hugo + GitHub Pages</span>
</a><a class=next href=https://codeofwhite.github.io/posts/llm/pruning-%E5%90%8E%E7%9A%84%E5%B7%A5%E4%BD%9C/><span class=title>Next »</span><br><span>Pruning 后的工作</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://codeofwhite.github.io/>CodeMist · 记录技术与生活</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script data-goatcounter=https://codeofwhite.goatcounter.com/count async src=//gc.zgo.at/count.js></script></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>